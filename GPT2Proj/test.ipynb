{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:34.442808700Z",
     "start_time": "2023-10-16T12:36:30.466112900Z"
    }
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import data_set\n",
    "import generate_sample\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "gpt_path = r\".\\pretrain_model\"\n",
    "train_path = r\".\\data\\train.json\"\n",
    "test_path = r\".\\data\\test.json\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:34.466744400Z",
     "start_time": "2023-10-16T12:36:34.444804300Z"
    }
   },
   "id": "cc0c23694126c409"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tk = transformers.BertTokenizer.from_pretrained(gpt_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:34.531571200Z",
     "start_time": "2023-10-16T12:36:34.459769Z"
    }
   },
   "id": "bc01d5a862b47e3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "GPT2 = model.GPT2LMHeadModel.from_pretrained(gpt_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:35.674656200Z",
     "start_time": "2023-10-16T12:36:34.508634100Z"
    }
   },
   "id": "1e09b22f3942c091"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(21128, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:36.139014500Z",
     "start_time": "2023-10-16T12:36:35.676651100Z"
    }
   },
   "id": "54dbb441d0c17cf0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Layer (type)                            Output Shape              Param #\n",
      "==========================================================================\n",
      "Embedding-1                              [4, 8, 768]           16,226,304\n",
      "Embedding-2                              [4, 8, 768]              786,432\n",
      "Dropout-3                                [4, 8, 768]                    0\n",
      "LayerNorm-4                              [4, 8, 768]                1,536\n",
      "Conv1D-5                                [4, 8, 2304]            1,771,776\n",
      "Dropout-6                              [4, 12, 8, 8]                    0\n",
      "Conv1D-7                                 [4, 8, 768]              590,592\n",
      "Dropout-8                                [4, 8, 768]                    0\n",
      "LayerNorm-9                              [4, 8, 768]                1,536\n",
      "Conv1D-10                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-11                    [4, 8, 3072]                    0\n",
      "Conv1D-12                                [4, 8, 768]            2,360,064\n",
      "Dropout-13                               [4, 8, 768]                    0\n",
      "LayerNorm-14                             [4, 8, 768]                1,536\n",
      "Conv1D-15                               [4, 8, 2304]            1,771,776\n",
      "Dropout-16                             [4, 12, 8, 8]                    0\n",
      "Conv1D-17                                [4, 8, 768]              590,592\n",
      "Dropout-18                               [4, 8, 768]                    0\n",
      "LayerNorm-19                             [4, 8, 768]                1,536\n",
      "Conv1D-20                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-21                    [4, 8, 3072]                    0\n",
      "Conv1D-22                                [4, 8, 768]            2,360,064\n",
      "Dropout-23                               [4, 8, 768]                    0\n",
      "LayerNorm-24                             [4, 8, 768]                1,536\n",
      "Conv1D-25                               [4, 8, 2304]            1,771,776\n",
      "Dropout-26                             [4, 12, 8, 8]                    0\n",
      "Conv1D-27                                [4, 8, 768]              590,592\n",
      "Dropout-28                               [4, 8, 768]                    0\n",
      "LayerNorm-29                             [4, 8, 768]                1,536\n",
      "Conv1D-30                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-31                    [4, 8, 3072]                    0\n",
      "Conv1D-32                                [4, 8, 768]            2,360,064\n",
      "Dropout-33                               [4, 8, 768]                    0\n",
      "LayerNorm-34                             [4, 8, 768]                1,536\n",
      "Conv1D-35                               [4, 8, 2304]            1,771,776\n",
      "Dropout-36                             [4, 12, 8, 8]                    0\n",
      "Conv1D-37                                [4, 8, 768]              590,592\n",
      "Dropout-38                               [4, 8, 768]                    0\n",
      "LayerNorm-39                             [4, 8, 768]                1,536\n",
      "Conv1D-40                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-41                    [4, 8, 3072]                    0\n",
      "Conv1D-42                                [4, 8, 768]            2,360,064\n",
      "Dropout-43                               [4, 8, 768]                    0\n",
      "LayerNorm-44                             [4, 8, 768]                1,536\n",
      "Conv1D-45                               [4, 8, 2304]            1,771,776\n",
      "Dropout-46                             [4, 12, 8, 8]                    0\n",
      "Conv1D-47                                [4, 8, 768]              590,592\n",
      "Dropout-48                               [4, 8, 768]                    0\n",
      "LayerNorm-49                             [4, 8, 768]                1,536\n",
      "Conv1D-50                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-51                    [4, 8, 3072]                    0\n",
      "Conv1D-52                                [4, 8, 768]            2,360,064\n",
      "Dropout-53                               [4, 8, 768]                    0\n",
      "LayerNorm-54                             [4, 8, 768]                1,536\n",
      "Conv1D-55                               [4, 8, 2304]            1,771,776\n",
      "Dropout-56                             [4, 12, 8, 8]                    0\n",
      "Conv1D-57                                [4, 8, 768]              590,592\n",
      "Dropout-58                               [4, 8, 768]                    0\n",
      "LayerNorm-59                             [4, 8, 768]                1,536\n",
      "Conv1D-60                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-61                    [4, 8, 3072]                    0\n",
      "Conv1D-62                                [4, 8, 768]            2,360,064\n",
      "Dropout-63                               [4, 8, 768]                    0\n",
      "LayerNorm-64                             [4, 8, 768]                1,536\n",
      "Conv1D-65                               [4, 8, 2304]            1,771,776\n",
      "Dropout-66                             [4, 12, 8, 8]                    0\n",
      "Conv1D-67                                [4, 8, 768]              590,592\n",
      "Dropout-68                               [4, 8, 768]                    0\n",
      "LayerNorm-69                             [4, 8, 768]                1,536\n",
      "Conv1D-70                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-71                    [4, 8, 3072]                    0\n",
      "Conv1D-72                                [4, 8, 768]            2,360,064\n",
      "Dropout-73                               [4, 8, 768]                    0\n",
      "LayerNorm-74                             [4, 8, 768]                1,536\n",
      "Conv1D-75                               [4, 8, 2304]            1,771,776\n",
      "Dropout-76                             [4, 12, 8, 8]                    0\n",
      "Conv1D-77                                [4, 8, 768]              590,592\n",
      "Dropout-78                               [4, 8, 768]                    0\n",
      "LayerNorm-79                             [4, 8, 768]                1,536\n",
      "Conv1D-80                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-81                    [4, 8, 3072]                    0\n",
      "Conv1D-82                                [4, 8, 768]            2,360,064\n",
      "Dropout-83                               [4, 8, 768]                    0\n",
      "LayerNorm-84                             [4, 8, 768]                1,536\n",
      "Conv1D-85                               [4, 8, 2304]            1,771,776\n",
      "Dropout-86                             [4, 12, 8, 8]                    0\n",
      "Conv1D-87                                [4, 8, 768]              590,592\n",
      "Dropout-88                               [4, 8, 768]                    0\n",
      "LayerNorm-89                             [4, 8, 768]                1,536\n",
      "Conv1D-90                               [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-91                    [4, 8, 3072]                    0\n",
      "Conv1D-92                                [4, 8, 768]            2,360,064\n",
      "Dropout-93                               [4, 8, 768]                    0\n",
      "LayerNorm-94                             [4, 8, 768]                1,536\n",
      "Conv1D-95                               [4, 8, 2304]            1,771,776\n",
      "Dropout-96                             [4, 12, 8, 8]                    0\n",
      "Conv1D-97                                [4, 8, 768]              590,592\n",
      "Dropout-98                               [4, 8, 768]                    0\n",
      "LayerNorm-99                             [4, 8, 768]                1,536\n",
      "Conv1D-100                              [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-101                   [4, 8, 3072]                    0\n",
      "Conv1D-102                               [4, 8, 768]            2,360,064\n",
      "Dropout-103                              [4, 8, 768]                    0\n",
      "LayerNorm-104                            [4, 8, 768]                1,536\n",
      "Conv1D-105                              [4, 8, 2304]            1,771,776\n",
      "Dropout-106                            [4, 12, 8, 8]                    0\n",
      "Conv1D-107                               [4, 8, 768]              590,592\n",
      "Dropout-108                              [4, 8, 768]                    0\n",
      "LayerNorm-109                            [4, 8, 768]                1,536\n",
      "Conv1D-110                              [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-111                   [4, 8, 3072]                    0\n",
      "Conv1D-112                               [4, 8, 768]            2,360,064\n",
      "Dropout-113                              [4, 8, 768]                    0\n",
      "LayerNorm-114                            [4, 8, 768]                1,536\n",
      "Conv1D-115                              [4, 8, 2304]            1,771,776\n",
      "Dropout-116                            [4, 12, 8, 8]                    0\n",
      "Conv1D-117                               [4, 8, 768]              590,592\n",
      "Dropout-118                              [4, 8, 768]                    0\n",
      "LayerNorm-119                            [4, 8, 768]                1,536\n",
      "Conv1D-120                              [4, 8, 3072]            2,362,368\n",
      "NewGELUActivation-121                   [4, 8, 3072]                    0\n",
      "Conv1D-122                               [4, 8, 768]            2,360,064\n",
      "Dropout-123                              [4, 8, 768]                    0\n",
      "LayerNorm-124                            [4, 8, 768]                1,536\n",
      "Linear-125                             [4, 8, 21128]           16,226,304\n",
      "==========================================================================\n",
      "Total params: 118,295,040\n",
      "Trainable params: 118,295,040\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "Input size (MB): 0.000069\n",
      "Forward/backward pass size (MB): 44.439453\n",
      "Params size (MB): 451.259766\n",
      "Estimated Total Size (MB): 495.699287\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "'--------------------------------------------------------------------------\\nLayer (type)                            Output Shape              Param #\\n==========================================================================\\nEmbedding-1                              [4, 8, 768]           16,226,304\\nEmbedding-2                              [4, 8, 768]              786,432\\nDropout-3                                [4, 8, 768]                    0\\nLayerNorm-4                              [4, 8, 768]                1,536\\nConv1D-5                                [4, 8, 2304]            1,771,776\\nDropout-6                              [4, 12, 8, 8]                    0\\nConv1D-7                                 [4, 8, 768]              590,592\\nDropout-8                                [4, 8, 768]                    0\\nLayerNorm-9                              [4, 8, 768]                1,536\\nConv1D-10                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-11                    [4, 8, 3072]                    0\\nConv1D-12                                [4, 8, 768]            2,360,064\\nDropout-13                               [4, 8, 768]                    0\\nLayerNorm-14                             [4, 8, 768]                1,536\\nConv1D-15                               [4, 8, 2304]            1,771,776\\nDropout-16                             [4, 12, 8, 8]                    0\\nConv1D-17                                [4, 8, 768]              590,592\\nDropout-18                               [4, 8, 768]                    0\\nLayerNorm-19                             [4, 8, 768]                1,536\\nConv1D-20                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-21                    [4, 8, 3072]                    0\\nConv1D-22                                [4, 8, 768]            2,360,064\\nDropout-23                               [4, 8, 768]                    0\\nLayerNorm-24                             [4, 8, 768]                1,536\\nConv1D-25                               [4, 8, 2304]            1,771,776\\nDropout-26                             [4, 12, 8, 8]                    0\\nConv1D-27                                [4, 8, 768]              590,592\\nDropout-28                               [4, 8, 768]                    0\\nLayerNorm-29                             [4, 8, 768]                1,536\\nConv1D-30                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-31                    [4, 8, 3072]                    0\\nConv1D-32                                [4, 8, 768]            2,360,064\\nDropout-33                               [4, 8, 768]                    0\\nLayerNorm-34                             [4, 8, 768]                1,536\\nConv1D-35                               [4, 8, 2304]            1,771,776\\nDropout-36                             [4, 12, 8, 8]                    0\\nConv1D-37                                [4, 8, 768]              590,592\\nDropout-38                               [4, 8, 768]                    0\\nLayerNorm-39                             [4, 8, 768]                1,536\\nConv1D-40                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-41                    [4, 8, 3072]                    0\\nConv1D-42                                [4, 8, 768]            2,360,064\\nDropout-43                               [4, 8, 768]                    0\\nLayerNorm-44                             [4, 8, 768]                1,536\\nConv1D-45                               [4, 8, 2304]            1,771,776\\nDropout-46                             [4, 12, 8, 8]                    0\\nConv1D-47                                [4, 8, 768]              590,592\\nDropout-48                               [4, 8, 768]                    0\\nLayerNorm-49                             [4, 8, 768]                1,536\\nConv1D-50                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-51                    [4, 8, 3072]                    0\\nConv1D-52                                [4, 8, 768]            2,360,064\\nDropout-53                               [4, 8, 768]                    0\\nLayerNorm-54                             [4, 8, 768]                1,536\\nConv1D-55                               [4, 8, 2304]            1,771,776\\nDropout-56                             [4, 12, 8, 8]                    0\\nConv1D-57                                [4, 8, 768]              590,592\\nDropout-58                               [4, 8, 768]                    0\\nLayerNorm-59                             [4, 8, 768]                1,536\\nConv1D-60                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-61                    [4, 8, 3072]                    0\\nConv1D-62                                [4, 8, 768]            2,360,064\\nDropout-63                               [4, 8, 768]                    0\\nLayerNorm-64                             [4, 8, 768]                1,536\\nConv1D-65                               [4, 8, 2304]            1,771,776\\nDropout-66                             [4, 12, 8, 8]                    0\\nConv1D-67                                [4, 8, 768]              590,592\\nDropout-68                               [4, 8, 768]                    0\\nLayerNorm-69                             [4, 8, 768]                1,536\\nConv1D-70                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-71                    [4, 8, 3072]                    0\\nConv1D-72                                [4, 8, 768]            2,360,064\\nDropout-73                               [4, 8, 768]                    0\\nLayerNorm-74                             [4, 8, 768]                1,536\\nConv1D-75                               [4, 8, 2304]            1,771,776\\nDropout-76                             [4, 12, 8, 8]                    0\\nConv1D-77                                [4, 8, 768]              590,592\\nDropout-78                               [4, 8, 768]                    0\\nLayerNorm-79                             [4, 8, 768]                1,536\\nConv1D-80                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-81                    [4, 8, 3072]                    0\\nConv1D-82                                [4, 8, 768]            2,360,064\\nDropout-83                               [4, 8, 768]                    0\\nLayerNorm-84                             [4, 8, 768]                1,536\\nConv1D-85                               [4, 8, 2304]            1,771,776\\nDropout-86                             [4, 12, 8, 8]                    0\\nConv1D-87                                [4, 8, 768]              590,592\\nDropout-88                               [4, 8, 768]                    0\\nLayerNorm-89                             [4, 8, 768]                1,536\\nConv1D-90                               [4, 8, 3072]            2,362,368\\nNewGELUActivation-91                    [4, 8, 3072]                    0\\nConv1D-92                                [4, 8, 768]            2,360,064\\nDropout-93                               [4, 8, 768]                    0\\nLayerNorm-94                             [4, 8, 768]                1,536\\nConv1D-95                               [4, 8, 2304]            1,771,776\\nDropout-96                             [4, 12, 8, 8]                    0\\nConv1D-97                                [4, 8, 768]              590,592\\nDropout-98                               [4, 8, 768]                    0\\nLayerNorm-99                             [4, 8, 768]                1,536\\nConv1D-100                              [4, 8, 3072]            2,362,368\\nNewGELUActivation-101                   [4, 8, 3072]                    0\\nConv1D-102                               [4, 8, 768]            2,360,064\\nDropout-103                              [4, 8, 768]                    0\\nLayerNorm-104                            [4, 8, 768]                1,536\\nConv1D-105                              [4, 8, 2304]            1,771,776\\nDropout-106                            [4, 12, 8, 8]                    0\\nConv1D-107                               [4, 8, 768]              590,592\\nDropout-108                              [4, 8, 768]                    0\\nLayerNorm-109                            [4, 8, 768]                1,536\\nConv1D-110                              [4, 8, 3072]            2,362,368\\nNewGELUActivation-111                   [4, 8, 3072]                    0\\nConv1D-112                               [4, 8, 768]            2,360,064\\nDropout-113                              [4, 8, 768]                    0\\nLayerNorm-114                            [4, 8, 768]                1,536\\nConv1D-115                              [4, 8, 2304]            1,771,776\\nDropout-116                            [4, 12, 8, 8]                    0\\nConv1D-117                               [4, 8, 768]              590,592\\nDropout-118                              [4, 8, 768]                    0\\nLayerNorm-119                            [4, 8, 768]                1,536\\nConv1D-120                              [4, 8, 3072]            2,362,368\\nNewGELUActivation-121                   [4, 8, 3072]                    0\\nConv1D-122                               [4, 8, 768]            2,360,064\\nDropout-123                              [4, 8, 768]                    0\\nLayerNorm-124                            [4, 8, 768]                1,536\\nLinear-125                             [4, 8, 21128]           16,226,304\\n==========================================================================\\nTotal params: 118,295,040\\nTrainable params: 118,295,040\\nNon-trainable params: 0\\n--------------------------------------------------------------------------\\nInput size (MB): 0.000069\\nForward/backward pass size (MB): 44.439453\\nParams size (MB): 451.259766\\nEstimated Total Size (MB): 495.699287\\n--------------------------------------------------------------------------'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchkeras\n",
    "import torch\n",
    "ids = torch.arange(0,32,device=\"cuda\").view(-1,8)\n",
    "\n",
    "torchkeras.summary(GPT2,ids,batch_size=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:38.327236900Z",
     "start_time": "2023-10-16T12:36:36.131043200Z"
    }
   },
   "id": "a814da24a9b3dbab"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "res = GPT2.__call__(ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:38.400086400Z",
     "start_time": "2023-10-16T12:36:38.326239800Z"
    }
   },
   "id": "14d7a7b6fc7568e5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "trainSet = data_set.GPT2DataSet(\n",
    "    tokenizer=tk,\n",
    "    max_len=512,\n",
    "    title_max_len=64,\n",
    "    data_dir=\".\\data\",\n",
    "    data_set_name=\"train\",\n",
    "    path_file=\".\\data\\\\train.json\"    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:40.513156700Z",
     "start_time": "2023-10-16T12:36:38.373116400Z"
    }
   },
   "id": "d75c1dd8f3917b22"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_usage()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:40.596068300Z",
     "start_time": "2023-10-16T12:36:40.515141300Z"
    }
   },
   "id": "f5b89bdbc9f042f8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 随 着 信 息 技 术 的 发 展, 微 博 正 在 被 广 泛 地 运 用 到 教 育 领 域. 微 博 教 育 是 否 适 用 于 高 校 公 共 艺 术 教 育 呢 ？ 文 章 以 高 校 公 共 艺 术 教 育 微 博 教 育 为 研 究 对 象, 通 过 对 公 共 艺 术 课 程 微 博 教 育 应 用 现 状 的 调 查, 从 基 于 微 博 视 域 的 国 内 外 教 育 应 用 研 究 现 状 、 微 博 视 域 下 艺 术 学 科 的 微 博 教 育 现 状 、 高 校 公 共 艺 术 教 育 课 程 应 用 微 博 教 学 的 可 行 性 分 析 、 微 博 视 角 下 高 校 公 共 艺 术 教 育 课 程 教 学 模 式 设 计 等 四 个 方 面 对 微 博 在 高 校 公 共 艺 术 教 育 中 的 可 行 性 进 行 分 析. [SEP] 高 校 公 共 艺 术 教 育 微 博 教 育 的 可 行 性 研 究 [SEP]\n",
      "微\n",
      "博\n",
      "教\n",
      "育\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m---> 20\u001B[0m     GPTOUT\u001B[38;5;241m=\u001B[39m \u001B[43mGPT2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpkvs\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     last_states,pkvs \u001B[38;5;241m=\u001B[39m GPTOUT\n\u001B[0;32m     26\u001B[0m     logits \u001B[38;5;241m=\u001B[39m last_states[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,:]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\ChatGPTBook\\GPT2Proj\\model.py:50\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.forward\u001B[1;34m(self, input_ids, labels, mask, past_key_values)\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;124;03m前向函数，计算GPT2预测结果值\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m \n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# 获取GPT2模型的输出结果\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# 获取GPT2模型的最后一层的隐层节点状态，size:[batch_size, sequence_length, config.n_embd]\u001B[39;00m\n\u001B[0;32m     52\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:900\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    890\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    891\u001B[0m         create_custom_forward(block),\n\u001B[0;32m    892\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    897\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    898\u001B[0m     )\n\u001B[0;32m    899\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 900\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    905\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    907\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    908\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    909\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    911\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:391\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[0;32m    389\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    390\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[1;32m--> 391\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    399\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[0;32m    400\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:332\u001B[0m, in \u001B[0;36mGPT2Attention.forward\u001B[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[0;32m    330\u001B[0m     attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001B[0;32m    331\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 332\u001B[0m     attn_output, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    334\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_heads(attn_output, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[0;32m    335\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_proj(attn_output)\n",
      "File \u001B[1;32mD:\\miniconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:202\u001B[0m, in \u001B[0;36mGPT2Attention._attn\u001B[1;34m(self, query, key, value, attention_mask, head_mask)\u001B[0m\n\u001B[0;32m    199\u001B[0m     mask_value \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfinfo(attn_weights\u001B[38;5;241m.\u001B[39mdtype)\u001B[38;5;241m.\u001B[39mmin\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001B[39;00m\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001B[39;00m\n\u001B[1;32m--> 202\u001B[0m     mask_value \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_weights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_weights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    203\u001B[0m     attn_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(causal_mask, attn_weights\u001B[38;5;241m.\u001B[39mto(attn_weights\u001B[38;5;241m.\u001B[39mdtype), mask_value)\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;66;03m# Apply the attention mask\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "samples = [trainSet[i] for i in range(10,11)]\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "batch = data_set.collate_func(samples)\n",
    "print(tk.decode(samples[0][\"input_ids\"]))\n",
    "res = GPT2.forward(\n",
    "    input_ids=batch[\"input_ids\"].cuda(),\n",
    "    labels=batch[\"input_ids\"].cuda(),\n",
    "    mask=batch[\"mask\"].cuda()\n",
    ")\n",
    "\n",
    "input_ids = batch[\"input_ids\"].cuda()\n",
    "mask      = batch[\"mask\"].cuda()\n",
    "label     = batch[\"input_ids\"].cuda()\n",
    "pkvs      = None\n",
    "import time\n",
    "t1 = time.time()\n",
    "torch.cuda.synchronize()\n",
    "for i in range(10):\n",
    "    GPTOUT= GPT2.forward(\n",
    "        input_ids=input_ids,\n",
    "        mask = mask,\n",
    "        past_key_values=pkvs\n",
    "    )\n",
    "    last_states,pkvs = GPTOUT\n",
    "    logits = last_states[...,-1,:]\n",
    "    next_token_ids = logits.argmax(-1).unsqueeze(1)\n",
    "    token = tk.decode([next_token_ids.item()])\n",
    "    print(token)\n",
    "    input_ids = torch.concat([input_ids,next_token_ids],dim=-1)\n",
    "\n",
    "print(f\"time usage:{time.time()-t1}\")\n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-16T12:36:43.008653500Z",
     "start_time": "2023-10-16T12:36:40.583107100Z"
    }
   },
   "id": "8abb66b29053b623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_token_ids.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-16T12:36:43.001703600Z"
    }
   },
   "id": "37096aeb0a99f7e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
